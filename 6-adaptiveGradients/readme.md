<b>MATH6001 Nonlinear Optimization in Machine Learning at Missouri S&T</b>

6. Adaptive Gradient Methods

Use Tensorflow (either v1.14 or v2.0), build a relatively small network and train the model on a small dataset, such as MNIST, via adaptive gradient methods: AdaGrad, RMSProp, AdaDelta, Adam. Plot the training error and test error as functions of training epoch. Test different hyperparameters. You can refer here for the use of optimizers in tensorflow: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers
